#!/bin/bash
#SBATCH --qos=preemptable
###SBATCH --qos=blanca-appm
#SBATCH --nodes=1
#SBATCH --ntasks=24
#SBATCH --time=2:00:00
#SBATCH --job-name=dedalus
#SBATCH --output=dedalus_output_%j
###SBATCH --mem=16g
#SBATCH --mail-type=begin
#SBATCH --mail-type=end
#SBATCH --mail-user=chli3324@colorado.edu
export SLURM_EXPORT_ENV=ALL
#export I_MPI_FABRICS=shm,tcp

##the slurm number to restart simulation... This need full state to be stored.
SUBMITDIR=$SLURM_SUBMIT_DIR
WORKDIR=/rc_scratch/chli3324/dedalus_$SLURM_JOB_ID
mkdir -p "$WORKDIR" && cp -r dedalus*.py "$WORKDIR" && cp -r dedalus.cfg "$WORKDIR" && cp submit_dedalus "$WORKDIR" && cd "$WORKDIR" || exit -1

###uncomment this line and change the folder number to select which simulation to restart
#ln -s /rc_scratch/chli3324/dedalus_13209996/analysis/analysis_s1.h5 restart.h5
#ln -s /rc_scratch/chli3324/dedalus_12808427/analysis/analysis_s1.h5 restart.h5
#ln -s /rc_scratch/chli3324/dedalus_13146434/analysis/analysis_s1.h5 restart.h5

source /curc/sw/anaconda3/latest
conda activate dedalus

##For IVP simulation
mpiexec -n $SLURM_NTASKS python3 dedalus_main.py
##for BVP, run in series
#python3 dedalus_main.py

#mpiexec -n $SLURM_NTASKS python3 dedalus_test.py
#python3 dedalus_test.py
#srun -n $SLURM_NTASKS 
#srun -N 2 -n 24 python3 dedalus_test.py
#srun -N $SLURM_NNODES -n $SLURM_NTASKS python3 dedalus_main.py
#srun -N $SLURM_NNODES --ntasks-per-node=$SLURM_NTASKS

cd "$SUBMITDIR" && cp dedalus_output_$SLURM_JOB_ID "$WORKDIR"



#cp /projects/chli3324/dedalus_main/dedalus_$SLURM_JOB_ID "$WORKDIR"
#python3 CGL_neumann.py
#python3 IFSC_2D.py
#mpiexec -n 24 python3 IFSC_2D.py
#mpiexec -n 24 python3 rayleigh_benard_3D.py
#mpiexec -n 24 python3 -m dedalus merge_procs snapshots
#mpiexec -n 24 python3 plot_slices.py snapshots/*.h5



